const express = require('express');
const OpenAI = require('openai');
const router = express.Router();

// Inicializar cliente de OpenAI
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Validaci√≥n de entrada
const validateChatRequest = (req, res, next) => {
  const { message, model, temperature, max_tokens } = req.body;
  
  if (!message || typeof message !== 'string' || message.trim().length === 0) {
    return res.status(400).json({
      error: 'Mensaje requerido',
      message: 'El campo "message" es obligatorio y debe ser una cadena no vac√≠a'
    });
  }
  
  if (message.length > 4000) {
    return res.status(400).json({
      error: 'Mensaje demasiado largo',
      message: 'El mensaje no puede exceder 4000 caracteres'
    });
  }
  
  // Validar par√°metros opcionales
  if (temperature && (temperature < 0 || temperature > 2)) {
    return res.status(400).json({
      error: 'Par√°metro inv√°lido',
      message: 'Temperature debe estar entre 0 y 2'
    });
  }
  
  if (max_tokens && (max_tokens < 1 || max_tokens > 4000)) {
    return res.status(400).json({
      error: 'Par√°metro inv√°lido',
      message: 'max_tokens debe estar entre 1 y 4000'
    });
  }
  
  next();
};

// Endpoint para chat
router.post('/chat', validateChatRequest, async (req, res) => {
  try {
     const { 
       message, 
       model = process.env.OPENAI_MODEL || 'gpt-3.5-turbo',
       temperature = 1,
       max_completion_tokens = 16000
     } = req.body;

    console.log(`üìù Nuevo mensaje recibido: ${message.substring(0, 100)}...`);

    const completion = await openai.chat.completions.create({
      model: model,
      messages: [
        {
          role: 'user',
          content: message
        }
      ],
      temperature: temperature,
      max_completion_tokens: max_completion_tokens,
    });

    const response = completion.choices[0]?.message?.content;

    console.log(completion.choices[0]);
    console.log(`üîç Respuesta recibida: ${response}`);

    if (!response) {
      throw new Error('No se recibi√≥ respuesta de OpenAI');
    }

    console.log(`‚úÖ Respuesta generada exitosamente`);

    res.json({
      success: true,
      data: {
        message: response,
        model: model,
        usage: completion.usage,
        timestamp: new Date().toISOString()
      }
    });

  } catch (error) {
    console.error('‚ùå Error en chat:', error);
    
    let statusCode = 500;
    let errorMessage = 'Error interno del servidor';
    
    if (error.code === 'insufficient_quota') {
      statusCode = 402;
      errorMessage = 'Cuota de API insuficiente';
    } else if (error.code === 'invalid_api_key') {
      statusCode = 401;
      errorMessage = 'API key inv√°lida';
    } else if (error.code === 'rate_limit_exceeded') {
      statusCode = 429;
      errorMessage = 'L√≠mite de velocidad excedido';
    } else if (error.message.includes('timeout')) {
      statusCode = 408;
      errorMessage = 'Timeout en la solicitud';
    }
    
    res.status(statusCode).json({
      success: false,
      error: errorMessage,
      message: error.message,
      timestamp: new Date().toISOString()
    });
  }
});

// Endpoint para obtener modelos disponibles
router.get('/models', async (req, res) => {
  try {
    const models = await openai.models.list();
    
    const availableModels = models.data
      .filter(model => model.id.includes('gpt'))
      .map(model => ({
        id: model.id,
        owned_by: model.owned_by,
        created: model.created
      }))
      .sort((a, b) => b.created - a.created);

    res.json({
      success: true,
      data: {
        models: availableModels,
        count: availableModels.length,
        timestamp: new Date().toISOString()
      }
    });

  } catch (error) {
    console.error('‚ùå Error obteniendo modelos:', error);
    
    res.status(500).json({
      success: false,
      error: 'Error obteniendo modelos',
      message: error.message,
      timestamp: new Date().toISOString()
    });
  }
});

// Endpoint para verificar configuraci√≥n
router.get('/status', (req, res) => {
  const hasApiKey = !!process.env.OPENAI_API_KEY;
  const model = process.env.OPENAI_MODEL || 'gpt-3.5-turbo';
  
  res.json({
    success: true,
    data: {
      configured: hasApiKey,
      model: model,
      environment: process.env.NODE_ENV || 'development',
      timestamp: new Date().toISOString()
    }
  });
});

module.exports = router;
